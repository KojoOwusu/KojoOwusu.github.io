define({ entries : {
    "DBLP:journals/corr/abs-2107-07045": {
        "abstract": "Explainable Artificial Intelligence (XAI) is an emerging area of research in the field of Artificial Intelligence (AI). XAI can explain how AI obtained a particular solution (e.g., classification or object detection) and can also answer other \"wh\" questions. This explainability is not possible in traditional AI. Explainability is essential for critical applications, such as defense, health care, law and order, and autonomous driving vehicles, etc, where the know-how is required for trust and transparency. A number of XAI techniques so far have been purposed for such applications. This paper provides an overview of these techniques from a multimedia (i.e., text, image, audio, and video) point of view. The advantages and shortcomings of these techniques have been discussed, and pointers to some future directions have also been provided.",
        "author": "Prashant Gohel and Priyanka Singh and Manoranjan Mohanty",
        "bibsource": "dblp computer science bibliography, https://dblp.org",
        "biburl": "https://dblp.org/rec/journals/corr/abs-2107-07045.bib",
        "doi": "https://doi.org/10.48550/arXiv.2107.07045",
        "eprint": "2107.07045",
        "eprinttype": "arXiv",
        "journal": "CoRR",
        "keywords": "type:Machine Learning,Artificial Intelligence",
        "timestamp": "Wed, 21 Jul 2021 15:55:35 +0200",
        "title": "Explainable {AI:} current status and future directions",
        "type": "article",
        "url": "https://arxiv.org/abs/2107.07045",
        "volume": "abs/2107.07045",
        "year": "2021"
    },
    "Yang_2019": {
        "abstract": "In this demo paper, we present the XFake system, an explainable fake news detector that assists end-users to identify news credibility. To effectively detect and interpret the fakeness of news items, we jointly consider both attributes (e.g., speaker) and statements. Specifically, MIMIC, ATTN and PERT frameworks are designed, where MIMIC is built for attribute analysis, ATTN is for statement semantic analysis and PERT is for statement linguistic analysis. Beyond the explanations extracted from the designed frameworks, relevant supporting examples as well as visualization are further provided to facilitate the interpretation. Our implemented system is demonstrated on a real-world dataset crawled from PolitiFact, where thousands of verified political news have been collected.",
        "author": "Fan Yang and Shiva K. Pentyala and Sina Mohseni and Mengnan Du and Hao Yuan and Rhema Linder and Eric D. Ragan and Shuiwang Ji and Xia (Ben) Hu",
        "booktitle": "The World Wide Web Conference",
        "doi": "10.1145/3308558.3314119",
        "keywords": "type:Computation and Language, Machine learning",
        "month": "may",
        "publisher": "{ACM ,",
        "title": "{XFake}: Explainable Fake News Detector with Visualizations",
        "type": "inproceedings",
        "url": "https://doi.org/10.1145%2F3308558.3314119",
        "year": "2019,"
    },
    "Zhou_2020": {
        "abstract": "The explosive growth in fake news and its erosion to democracy, justice, and public trust has increased the demand for fake news detection and intervention. This survey reviews and evaluates methods that can detect fake news from four perspectives: (1) the false knowledge it carries, (2) its writing style, (3) its propagation patterns, and (4) the credibility of its source. The survey also highlights some potential research tasks based on the review. In particular, we identify and detail related fundamental theories across various disciplines to encourage interdisciplinary research on fake news. We hope this survey can facilitate collaborative efforts among experts in computer and information sciences, social sciences, political science, and journalism to research fake news, where such efforts can lead to fake news detection that is not only efficient but more importantly, explainable.",
        "author": "Xinyi Zhou and Reza Zafarani",
        "doi": "10.1145/3395046",
        "journal": "{ACM} Computing Surveys",
        "keywords": "type:Artificial Intelligence, Computation and language, Social and information networks",
        "month": "sep",
        "number": "5",
        "pages": "1--40",
        "publisher": "Association for Computing Machinery ({ACM})",
        "title": "A Survey of Fake News",
        "type": "article",
        "url": "https://doi.org/10.1145%2F3395046",
        "volume": "53",
        "year": "2020,"
    },
    "achtibat2022where": {
        "abstract": "The emerging field of eXplainable Artificial Intelligence (XAI) aims to bring transparency to today's powerful but opaque deep learning models. While local XAI methods explain individual predictions in form of attribution maps, thereby identifying where important features occur (but not providing information about what they represent), global explanation techniques visualize what concepts a model has generally learned to encode. Both types of methods thus only provide partial insights and leave the burden of interpreting the model's reasoning to the user. Only few contemporary techniques aim at combining the principles behind both local and global XAI for obtaining more informative explanations. Those methods, however, are often limited to specific model architectures or impose additional requirements on training regimes or data and label availability, which renders the post-hoc application to arbitrarily pre-trained models practically impossible. In this work we introduce the Concept Relevance Propagation (CRP) approach, which combines the local and global perspectives of XAI and thus allows answering both the \"where\" and \"what\" questions for individual predictions, without additional constraints imposed. We further introduce the principle of Relevance Maximization for finding representative examples of encoded concepts based on their usefulness to the model. We thereby lift the dependency on the common practice of Activation Maximization and its limitations. We demonstrate the capabilities of our methods in various settings, showcasing that Concept Relevance Propagation and Relevance Maximization lead to more human interpretable explanations and provide deep insights into the model's representations and reasoning through concept atlases, concept composition analyses, and quantitative investigations of concept subspaces and their role in fine-grained decision making.",
        "archiveprefix": "arXiv",
        "author": "Reduan Achtibat and Maximilian Dreyer and Ilona Eisenbraun and Sebastian Bosse and Thomas Wiegand and Wojciech Samek and Sebastian Lapuschkin",
        "doi": "https://doi.org/10.48550/arXiv.2206.03208",
        "eprint": "2206.03208",
        "keywords": "type:Machine learning, Artificial Intelligence",
        "primaryclass": "cs.LG",
        "title": "From \"Where\" to \"What\": Towards Human-Understandable Explanations through Concept Relevance Propagation",
        "type": "misc",
        "url": "https://arxiv.org/abs/2206.03208",
        "year": "2022"
    },
    "https://doi.org/10.1002/ett.3767": {
        "abstract": "Abstract With the ever increase in social media usage, it has become necessary to combat the spread of false information and decrease the reliance of information retrieval from such sources. Social platforms are under constant pressure to come up with efficient methods to solve this problem because users' interaction with fake and unreliable news leads to its spread at an individual level. This spreading of misinformation adversely affects the perception about an important activity, and as such, it needs to be dealt with using a modern approach. In this paper, we collect 1356 news instances from various users via Twitter and media sources such as PolitiFact and create several datasets for the real and the fake news stories. Our study compares multiple state-of-the-art approaches such as convolutional neural networks (CNNs), long short-term memories (LSTMs), ensemble methods, and attention mechanisms. We conclude that CNN\u00a0+\u00a0bidirectional LSTM ensembled network with attention mechanism achieved the highest accuracy of 88.78\\%, whereas Ko et al tackled the fake news identification problem and achieved a detection rate of 85\\%.",
        "author": "Kumar, Sachin and Asthana, Rohan and Upadhyay, Shashwat and Upreti, Nidhi and Akbar, Mohammad",
        "doi": "https://doi.org/10.1002/ett.3767",
        "eprint": "https://onlinelibrary.wiley.com/doi/pdf/10.1002/ett.3767",
        "journal": "Transactions on Emerging Telecommunications Technologies",
        "keywords": "type:Artificial Intelligence, Machine Learning",
        "note": "e3767 ETT-19-0216.R1",
        "number": "2",
        "pages": "e3767",
        "title": "Fake news detection using deep learning models: A novel approach",
        "type": "article",
        "url": "https://onlinelibrary.wiley.com/doi/abs/10.1002/ett.3767",
        "volume": "31",
        "year": "2020"
    },
    "lundberg2017unified": {
        "abstract": "Understanding why a model makes a certain prediction can be as crucial as the prediction's accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, we present a unified framework for interpreting predictions, SHAP (SHapley Additive exPlanations). SHAP assigns each feature an importance value for a particular prediction. Its novel components include: (1) the identification of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties. The new class unifies six existing methods, notable because several recent methods in the class lack the proposed desirable properties. Based on insights from this unification, we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches.",
        "archiveprefix": "arXiv",
        "author": "Scott Lundberg and Su-In Lee",
        "doi": "https://doi.org/10.48550/arXiv.1705.07874",
        "eprint": "1705.07874",
        "keywords": "type:Artificial intelligence, Machine learning",
        "primaryclass": "cs.AI",
        "title": "A Unified Approach to Interpreting Model Predictions",
        "type": "misc",
        "year": "2017"
    },
    "mallet2023hybrid": {
        "abstract": "The growing reliance of society on social media for authentic information has done nothing but increase over the past years. This has only raised the potential consequences of the spread of misinformation. One of the growing methods in popularity is to deceive users using a deepfake. A deepfake is an invention that has come with the latest technological advancements, which enables nefarious online users to replace their face with a computer generated, synthetic face of numerous powerful members of society. Deepfake images and videos now provide the means to mimic important political and cultural figures to spread massive amounts of false information. Models that can detect these deepfakes to prevent the spread of misinformation are now of tremendous necessity. In this paper, we propose a new deepfake detection schema utilizing two deep learning algorithms: long short term memory and multilayer perceptron. We evaluate our model using a publicly available dataset named 140k Real and Fake Faces to detect images altered by a deepfake with accuracies achieved as high as 74.7%",
        "archiveprefix": "arXiv",
        "author": "Jacob Mallet and Natalie Krueger and Mounika Vanamala and Rushit Dave",
        "doi": "https://doi.org/10.48550/arXiv.2304.14504",
        "eprint": "2304.14504",
        "keywords": "type:Computer Vision, Pattern Recognition, Machine learning",
        "primaryclass": "cs.CV",
        "title": "Hybrid Deepfake Detection Utilizing MLP and LSTM",
        "type": "misc",
        "url": "https://arxiv.org/abs/2304.14504",
        "year": "2023"
    },
    "mohseni2020machine": {
        "abstract": "Combating fake news and misinformation propagation is a challenging task in the post-truth era. News feed and search algorithms could potentially lead to unintentional large-scale propagation of false and fabricated information with users being exposed to algorithmically selected false content. Our research investigates the effects of an Explainable AI assistant embedded in news review platforms for combating the propagation of fake news. We design a news reviewing and sharing interface, create a dataset of news stories, and train four interpretable fake news detection algorithms to study the effects of algorithmic transparency on end-users. We present evaluation results and analysis from multiple controlled crowdsourced studies. For a deeper understanding of Explainable AI systems, we discuss interactions between user engagement, mental model, trust, and performance measures in the process of explaining. The study results indicate that explanations helped participants to build appropriate mental models of the intelligent assistants in different conditions and adjust their trust accordingly for model limitations.",
        "archiveprefix": "arXiv",
        "author": "Sina Mohseni and Fan Yang and Shiva Pentyala and Mengnan Du and Yi Liu and Nic Lupfer and Xia Hu and Shuiwang Ji and Eric Ragan",
        "doi": "https://doi.org/10.48550/arXiv.2007.12358",
        "eprint": "2007.12358",
        "keywords": "type:Artificial Intelligence,Social and Information Networks, Information Retrieval",
        "primaryclass": "cs.IR",
        "title": "Machine Learning Explanations to Prevent Overtrust in Fake News Detection",
        "type": "misc",
        "url": "https://arxiv.org/abs/2007.12358",
        "year": "2020"
    },
    "tjoa2023improving": {
        "abstract": "This paper quantifies the quality of heatmap-based eXplainable AI (XAI) methods w.r.t image classification problem. Here, a heatmap is considered desirable if it improves the probability of predicting the correct classes. Different XAI heatmap-based methods are empirically shown to improve classification confidence to different extents depending on the datasets, e.g. Saliency works best on ImageNet and Deconvolution on Chest X-Ray Pneumonia dataset. The novelty includes a new gap distribution that shows a stark difference between correct and wrong predictions. Finally, the generative augmentative explanation is introduced, a method to generate heatmaps capable of improving predictive confidence to a high level.",
        "archiveprefix": "arXiv",
        "author": "Erico Tjoa and Hong Jing Khok and Tushar Chouhan and Guan Cuntai",
        "doi": "https://doi.org/10.48550/arXiv.2201.00009",
        "eprint": "2201.00009",
        "keywords": "type:Machine learning, Artificial intelligence",
        "primaryclass": "cs.LG",
        "title": "Improving Deep Neural Network Classification Confidence using Heatmap-based eXplainable AI",
        "type": "misc",
        "url": "https://arxiv.org/abs/2201.00009",
        "year": "2023"
    },
    "vu2020ceval": {
        "abstract": "In many modern image-classification applications, understanding the cause of model's prediction can be as critical as the prediction's accuracy itself. Various feature-based local explanations generation methods have been designed to give us more insights on the decision of complex classifiers. Nevertheless, there is no consensus on evaluating the quality of different explanations. In response to this lack of comprehensive evaluation, we introduce the c-Eval metric and its corresponding framework to quantify the feature-based local explanation's quality. Given a classifier's prediction and the corresponding explanation on that prediction, c-Eval is the minimum-distortion perturbation that successfully alters the prediction while keeping the explanation's features unchanged. We then demonstrate how c-Eval can be computed using some modifications on existing adversarial generation libraries. To show that c-Eval captures the importance of input's features, we establish the connection between c-Eval and the features returned by explainers in affine and nearly-affine classifiers. We then introduce the c-Eval plot, which not only displays a strong connection between c-Eval and explainers' quality, but also helps automatically determine explainer's parameters. Since the generation of c-Eval relies on adversarial generation, we provide a demo of c-Eval on adversarial-robust models and show that the metric is applicable in those models. Finally, extensive experiments of explainers on different datasets are conducted to support the adoption of c-Eval in evaluating explainers' performance.",
        "archiveprefix": "arXiv",
        "author": "Minh N. Vu and Truc D. Nguyen and NhatHai Phan and Ralucca Gera and My T. Thai",
        "doi": "https://doi.org/10.48550/arXiv.1906.02032",
        "eprint": "1906.02032",
        "keywords": "type:Machine learning",
        "primaryclass": "cs.LG",
        "title": "c-Eval: A Unified Metric to Evaluate Feature-based Explanations via Perturbation",
        "type": "misc",
        "url": "https://arxiv.org/abs/1906.02032",
        "year": "2020"
    }
}});