
@misc{vu2020ceval,
      title={c-Eval: A Unified Metric to Evaluate Feature-based Explanations via Perturbation},
      author={Minh N. Vu and Truc D. Nguyen and NhatHai Phan and Ralucca Gera and My T. Thai},
      abstract = {In many modern image-classification applications, understanding the cause of model's prediction can be as critical as the prediction's accuracy itself. Various feature-based local explanations generation methods have been designed to give us more insights on the decision of complex classifiers. Nevertheless, there is no consensus on evaluating the quality of different explanations. In response to this lack of comprehensive evaluation, we introduce the c-Eval metric and its corresponding framework to quantify the feature-based local explanation's quality. Given a classifier's prediction and the corresponding explanation on that prediction, c-Eval is the minimum-distortion perturbation that successfully alters the prediction while keeping the explanation's features unchanged. We then demonstrate how c-Eval can be computed using some modifications on existing adversarial generation libraries. To show that c-Eval captures the importance of input's features, we establish the connection between c-Eval and the features returned by explainers in affine and nearly-affine classifiers. We then introduce the c-Eval plot, which not only displays a strong connection between c-Eval and explainers' quality, but also helps automatically determine explainer's parameters. Since the generation of c-Eval relies on adversarial generation, we provide a demo of c-Eval on adversarial-robust models and show that the metric is applicable in those models. Finally, extensive experiments of explainers on different datasets are conducted to support the adoption of c-Eval in evaluating explainers' performance.},
      year={2020},
      doi={https://doi.org/10.48550/arXiv.1906.02032},
      keywords={type:Machine learning},
      eprint={1906.02032},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url = {https://arxiv.org/abs/1906.02032}
}
@misc{tjoa2023improving,
      title={Improving Deep Neural Network Classification Confidence using Heatmap-based eXplainable AI},
      author={Erico Tjoa and Hong Jing Khok and Tushar Chouhan and Guan Cuntai},
      year={2023},
      eprint={2201.00009},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      keywords={type:Machine learning, Artificial intelligence},
      doi={https://doi.org/10.48550/arXiv.2201.00009},
      url={https://arxiv.org/abs/2201.00009},
      abstract={This paper quantifies the quality of heatmap-based eXplainable AI (XAI) methods w.r.t image classification problem. Here, a heatmap is considered desirable if it improves the probability of predicting the correct classes. Different XAI heatmap-based methods are empirically shown to improve classification confidence to different extents depending on the datasets, e.g. Saliency works best on ImageNet and Deconvolution on Chest X-Ray Pneumonia dataset. The novelty includes a new gap distribution that shows a stark difference between correct and wrong predictions. Finally, the generative augmentative explanation is introduced, a method to generate heatmaps capable of improving predictive confidence to a high level.}
}
@inproceedings{Yang_2019,
	doi = {10.1145/3308558.3314119},

	url = {https://doi.org/10.1145%2F3308558.3314119},

	year = 2019,
	month = {may},

	publisher = {{ACM}
},

	author = {Fan Yang and Shiva K. Pentyala and Sina Mohseni and Mengnan Du and Hao Yuan and Rhema Linder and Eric D. Ragan and Shuiwang Ji and Xia (Ben) Hu},

	title = {{XFake}: Explainable Fake News Detector with Visualizations},

	booktitle = {The World Wide Web Conference},
  abstract = {In this demo paper, we present the XFake system, an explainable fake news detector that assists end-users to identify news credibility. To effectively detect and interpret the fakeness of news items, we jointly consider both attributes (e.g., speaker) and statements. Specifically, MIMIC, ATTN and PERT frameworks are designed, where MIMIC is built for attribute analysis, ATTN is for statement semantic analysis and PERT is for statement linguistic analysis. Beyond the explanations extracted from the designed frameworks, relevant supporting examples as well as visualization are further provided to facilitate the interpretation. Our implemented system is demonstrated on a real-world dataset crawled from PolitiFact, where thousands of verified political news have been collected.},
  keywords={type:Computation and Language, Machine learning}
}
@misc{mohseni2020machine,
      title={Machine Learning Explanations to Prevent Overtrust in Fake News Detection},
      author={Sina Mohseni and Fan Yang and Shiva Pentyala and Mengnan Du and Yi Liu and Nic Lupfer and Xia Hu and Shuiwang Ji and Eric Ragan},
      year={2020},
      abstract={Combating fake news and misinformation propagation is a challenging task in the post-truth era. News feed and search algorithms could potentially lead to unintentional large-scale propagation of false and fabricated information with users being exposed to algorithmically selected false content. Our research investigates the effects of an Explainable AI assistant embedded in news review platforms for combating the propagation of fake news. We design a news reviewing and sharing interface, create a dataset of news stories, and train four interpretable fake news detection algorithms to study the effects of algorithmic transparency on end-users. We present evaluation results and analysis from multiple controlled crowdsourced studies. For a deeper understanding of Explainable AI systems, we discuss interactions between user engagement, mental model, trust, and performance measures in the process of explaining. The study results indicate that explanations helped participants to build appropriate mental models of the intelligent assistants in different conditions and adjust their trust accordingly for model limitations.},
      doi={https://doi.org/10.48550/arXiv.2007.12358},
      url={https://arxiv.org/abs/2007.12358},
      keywords={type:Artificial Intelligence,Social and Information Networks, Information Retrieval},
      eprint={2007.12358},
      archivePrefix={arXiv},
      primaryClass={cs.IR}
}
@article{Zhou_2020,
	doi = {10.1145/3395046},

	url = {https://doi.org/10.1145%2F3395046},

	year = 2020,
	month = {sep},

	publisher = {Association for Computing Machinery ({ACM})},

	volume = {53},

	number = {5},

	pages = {1--40},

	author = {Xinyi Zhou and Reza Zafarani},

	title = {A Survey of Fake News},

	journal = {{ACM} Computing Surveys},
  keywords={type:Artificial Intelligence, Computation and language, Social and information networks},
  abstract={The explosive growth in fake news and its erosion to democracy, justice, and public trust has increased the demand for fake news detection and intervention. This survey reviews and evaluates methods that can detect fake news from four perspectives: (1) the false knowledge it carries, (2) its writing style, (3) its propagation patterns, and (4) the credibility of its source. The survey also highlights some potential research tasks based on the review. In particular, we identify and detail related fundamental theories across various disciplines to encourage interdisciplinary research on fake news. We hope this survey can facilitate collaborative efforts among experts in computer and information sciences, social sciences, political science, and journalism to research fake news, where such efforts can lead to fake news detection that is not only efficient but more importantly, explainable.}
}
@article{https://doi.org/10.1002/ett.3767,
author = {Kumar, Sachin and Asthana, Rohan and Upadhyay, Shashwat and Upreti, Nidhi and Akbar, Mohammad},
title = {Fake news detection using deep learning models: A novel approach},
journal = {Transactions on Emerging Telecommunications Technologies},
volume = {31},
number = {2},
pages = {e3767},
doi = {https://doi.org/10.1002/ett.3767},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/ett.3767},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/ett.3767},
note = {e3767 ETT-19-0216.R1},
abstract = {Abstract With the ever increase in social media usage, it has become necessary to combat the spread of false information and decrease the reliance of information retrieval from such sources. Social platforms are under constant pressure to come up with efficient methods to solve this problem because users' interaction with fake and unreliable news leads to its spread at an individual level. This spreading of misinformation adversely affects the perception about an important activity, and as such, it needs to be dealt with using a modern approach. In this paper, we collect 1356 news instances from various users via Twitter and media sources such as PolitiFact and create several datasets for the real and the fake news stories. Our study compares multiple state-of-the-art approaches such as convolutional neural networks (CNNs), long short-term memories (LSTMs), ensemble methods, and attention mechanisms. We conclude that CNN + bidirectional LSTM ensembled network with attention mechanism achieved the highest accuracy of 88.78\%, whereas Ko et al tackled the fake news identification problem and achieved a detection rate of 85\%.},
year = {2020},
keywords={type:Artificial Intelligence, Machine Learning}
}

@article{DBLP:journals/corr/abs-2107-07045,
  author       = {Prashant Gohel and
                  Priyanka Singh and
                  Manoranjan Mohanty},
  title        = {Explainable {AI:} current status and future directions},
  journal      = {CoRR},
  volume       = {abs/2107.07045},
  year         = {2021},
  url          = {https://arxiv.org/abs/2107.07045},
  eprinttype    = {arXiv},
  eprint       = {2107.07045},
  timestamp    = {Wed, 21 Jul 2021 15:55:35 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2107-07045.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  doi={https://doi.org/10.48550/arXiv.2107.07045},
  keywords={type:Machine Learning,Artificial Intelligence},
  abstract={Explainable Artificial Intelligence (XAI) is an emerging area of research in the field of Artificial Intelligence (AI). XAI can explain how AI obtained a particular solution (e.g., classification or object detection) and can also answer other "wh" questions. This explainability is not possible in traditional AI. Explainability is essential for critical applications, such as defense, health care, law and order, and autonomous driving vehicles, etc, where the know-how is required for trust and transparency. A number of XAI techniques so far have been purposed for such applications. This paper provides an overview of these techniques from a multimedia (i.e., text, image, audio, and video) point of view. The advantages and shortcomings of these techniques have been discussed, and pointers to some future directions have also been provided.}
}
@misc{achtibat2022where,
      title={From "Where" to "What": Towards Human-Understandable Explanations through Concept Relevance Propagation},
      author={Reduan Achtibat and Maximilian Dreyer and Ilona Eisenbraun and Sebastian Bosse and Thomas Wiegand and Wojciech Samek and Sebastian Lapuschkin},
      year={2022},
      url={https://arxiv.org/abs/2206.03208},
      doi={https://doi.org/10.48550/arXiv.2206.03208},
      abstract={The emerging field of eXplainable Artificial Intelligence (XAI) aims to bring transparency to today's powerful but opaque deep learning models. While local XAI methods explain individual predictions in form of attribution maps, thereby identifying where important features occur (but not providing information about what they represent), global explanation techniques visualize what concepts a model has generally learned to encode. Both types of methods thus only provide partial insights and leave the burden of interpreting the model's reasoning to the user. Only few contemporary techniques aim at combining the principles behind both local and global XAI for obtaining more informative explanations. Those methods, however, are often limited to specific model architectures or impose additional requirements on training regimes or data and label availability, which renders the post-hoc application to arbitrarily pre-trained models practically impossible. In this work we introduce the Concept Relevance Propagation (CRP) approach, which combines the local and global perspectives of XAI and thus allows answering both the "where" and "what" questions for individual predictions, without additional constraints imposed. We further introduce the principle of Relevance Maximization for finding representative examples of encoded concepts based on their usefulness to the model. We thereby lift the dependency on the common practice of Activation Maximization and its limitations. We demonstrate the capabilities of our methods in various settings, showcasing that Concept Relevance Propagation and Relevance Maximization lead to more human interpretable explanations and provide deep insights into the model's representations and reasoning through concept atlases, concept composition analyses, and quantitative investigations of concept subspaces and their role in fine-grained decision making.},
      keywords={type:Machine learning, Artificial Intelligence},
      eprint={2206.03208},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{mallet2023hybrid,
      title={Hybrid Deepfake Detection Utilizing MLP and LSTM},
      abstract={The growing reliance of society on social media for authentic information has done nothing but increase over the past years. This has only raised the potential consequences of the spread of misinformation. One of the growing methods in popularity is to deceive users using a deepfake. A deepfake is an invention that has come with the latest technological advancements, which enables nefarious online users to replace their face with a computer generated, synthetic face of numerous powerful members of society. Deepfake images and videos now provide the means to mimic important political and cultural figures to spread massive amounts of false information. Models that can detect these deepfakes to prevent the spread of misinformation are now of tremendous necessity. In this paper, we propose a new deepfake detection schema utilizing two deep learning algorithms: long short term memory and multilayer perceptron. We evaluate our model using a publicly available dataset named 140k Real and Fake Faces to detect images altered by a deepfake with accuracies achieved as high as 74.7%},
      author={Jacob Mallet and Natalie Krueger and Mounika Vanamala and Rushit Dave},
      year={2023},
      url={https://arxiv.org/abs/2304.14504},
      doi={https://doi.org/10.48550/arXiv.2304.14504},
      eprint={2304.14504},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      keywords={type:Computer Vision, Pattern Recognition, Machine learning}
}
@inproceedings{mosca-etal-2022-shap,
    title = {SHAP-Based Explanation Methods: A Review for {NLP} Interpretability},
    author = {Mosca, Edoardo  and
      Szigeti, Ferenc  and
      Tragianni, Stella  and
      Gallagher, Daniel  and
      Groh, Georg},
    booktitle = {Proceedings of the 29th International Conference on Computational Linguistics},
    month = {oct},
    year = {2022},
    address = {Gyeongju, Republic of Korea},
    publisher = {International Committee on Computational Linguistics},
    url = {https://aclanthology.org/2022.coling-1.406},
    pages = "4593--4603",
    abstract = {Model explanations are crucial for the transparent, safe, and trustworthy deployment of machine learning models. The SHapley Additive exPlanations (SHAP) framework is considered by many to be a gold standard for local explanations thanks to its solid theoretical background and general applicability. In the years following its publication, several variants appeared in the literature—presenting adaptations in the core assumptions and target applications. In this work, we review all relevant SHAP-based interpretability approaches available to date and provide instructive examples as well as recommendations regarding their applicability to NLP use cases.},
    keywords={type:Linguistics, Machine learning, Natural Language Processing, Artificial Intelligence}
}
